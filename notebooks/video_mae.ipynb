{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"../cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:33:18.207413: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 09:33:18.381475: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 09:33:19.431145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel, VideoMAEFeatureExtractor, AutoImageProcessor, VideoMAEForPreTraining, VideoMAEForVideoClassification\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\", cache_dir=CACHE_DIR)\n",
    "videomae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "num_frames = 32\n",
    "video = list(np.random.randint(0, 256, (num_frames, 3, 1024, 512)))\n",
    "\n",
    "model_input = processor([video, video], return_tensors=\"pt\", padding=True)\n",
    "pixel_values = model_input.pixel_values\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = videomae(pixel_values)\n",
    "print(outputs.keys())\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1568, 768])\n"
     ]
    }
   ],
   "source": [
    "sequence_output = outputs[0]\n",
    "batch_size, seq_len, hidden_size = sequence_output.shape\n",
    "print(sequence_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 98, 768])\n"
     ]
    }
   ],
   "source": [
    "video_sequence_output = sequence_output.reshape(batch_size, num_frames, -1, hidden_size)\n",
    "print(video_sequence_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# torch.Size([2, 16, 98, 768]) to torch.Size([2, 16, 768]) by compute mean\n",
    "video_mean_output = torch.mean(video_sequence_output, dim=2)\n",
    "print(video_mean_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEForPreTraining(\n",
       "  (videomae): VideoMAEModel(\n",
       "    (embeddings): VideoMAEEmbeddings(\n",
       "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): VideoMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (encoder_to_decoder): Linear(in_features=768, out_features=384, bias=False)\n",
       "  (decoder): VideoMAEDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-3): 4 x VideoMAELayer(\n",
       "        (attention): VideoMAEAttention(\n",
       "          (attention): VideoMAESelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VideoMAESelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VideoMAEIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VideoMAEOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=384, out_features=1536, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_video_mae = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\", cache_dir=CACHE_DIR)\n",
    "pretrain_video_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer, TransformerEncoderLayer, TransformerEncoder\n",
    "import math\n",
    "\n",
    "emb_size = 128\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = emb_size * 2\n",
    "dropout = 0.1\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(\n",
    "    emb_size, nhead, dim_feedforward, dropout, batch_first=False\n",
    ")\n",
    "transformer_encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "predictor = nn.Sequential(\n",
    "    nn.Linear(emb_size, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape: torch.Size([19, 1, 128])\n",
      "merged_output.shape: torch.Size([1, 128])\n",
      "predictions.shape: torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3660], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if batch_first=True, input shape should be (batch_size, seq_len, 2)\n",
    "# else, input shape should be (seq_len, batch_size, emb_size)\n",
    "seq_len = 19\n",
    "batch_size = 1\n",
    "inputs = torch.randn(seq_len, batch_size, emb_size)\n",
    "outputs = transformer_encoder(inputs)\n",
    "print(f'outputs.shape: {outputs.shape}')\n",
    "\n",
    "merged_output = outputs.mean(0)\n",
    "print(f'merged_output.shape: {merged_output.shape}')\n",
    "\n",
    "predictions = predictor(merged_output)\n",
    "print(f'predictions.shape: {predictions.shape}')\n",
    "predictions.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape: torch.Size([19, 1, 128])\n",
      "hidden_states.shape: torch.Size([19, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(\n",
    "            token_embedding + self.pos_embedding[: token_embedding.size(0), :]\n",
    "        )\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, emb_size, nhead, num_encoder_layers, dim_feedforward, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            emb_size, nhead, dim_feedforward, dropout\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        return self.encoder(src, src_mask, src_key_padding_mask)\n",
    "\n",
    "position_encoder = PositionalEncoding(emb_size, dropout)\n",
    "encoder = Encoder(emb_size, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "src = torch.randn(seq_len, batch_size, emb_size)\n",
    "src = position_encoder(src)\n",
    "print(f'src.shape: {src.shape}')\n",
    "\n",
    "hidden_states = encoder(src)\n",
    "print(f'hidden_states.shape: {hidden_states.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape: torch.Size([19, 1, 1])\n",
      "outputs.shape: torch.Size([19, 1])\n"
     ]
    }
   ],
   "source": [
    "classifier = nn.Linear(\n",
    "    emb_size, 1\n",
    ")\n",
    "\n",
    "outputs = classifier(hidden_states)\n",
    "print(f'outputs.shape: {outputs.shape}')\n",
    "\n",
    "outputs = outputs.squeeze(-1)\n",
    "print(f'outputs.shape: {outputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
