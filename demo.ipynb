{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "\n",
    "from model.layers import AttentiveDiscriminator, AttentiveSummarizer\n",
    "from utils import TensorboardWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1024\n",
    "hidden_size = 512\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "dim_feedforward = hidden_size * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhnam/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "linear_compress = nn.Linear(\n",
    "    input_size, hidden_size\n",
    ").cuda()\n",
    "\n",
    "## Summarizer: attentive_selector + attentive_auto_encoder\n",
    "summarizer = AttentiveSummarizer(\n",
    "    d_model=hidden_size,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    ").cuda()\n",
    "\n",
    "    ## Discriminator: attentive_discriminator\n",
    "discriminator = AttentiveDiscriminator(\n",
    "    d_model=hidden_size,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    ").cuda()\n",
    "\n",
    "model = nn.ModuleList(\n",
    "    [linear_compress, summarizer, discriminator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_dir = \"epoch-43.pt\"\n",
    "state_dict = torch.load(pretrained_model_dir)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GoogLeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhnam/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/minhnam/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "full_lenet_model = models.googlenet(pretrained=True)\n",
    "pool5_extractor = nn.Sequential(*list(full_lenet_model.children())[:-2]).cuda()\n",
    "pool5_extractor(torch.randn(1,3,224,224).cuda()).shape\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # Load the InceptionV1 (GoogLeNet) model\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=True)\n",
    "\n",
    "# # Create a new model that outputs the pool5 layer's output\n",
    "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "video_path = 'tvsum50_ver_1_1/ydata-tvsum50-v1_1/ydata-tvsum50-video/video/3eYKfiOEJNs.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "original_frames = []\n",
    "prerpocessed_frames = []\n",
    "\n",
    "# Create a transform for preprocessing frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Process each frame in the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break  # Break the loop if no more frames\n",
    "    \n",
    "    original_frames.append(frame)\n",
    "    \n",
    "    # Preprocess the frame\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = transform(frame)\n",
    "    prerpocessed_frames.append(frame)\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool5_extractor.cuda().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(prerpocessed_frames), 32):\n",
    "        batch = prerpocessed_frames[i:i+32]\n",
    "        batch = torch.stack(batch)\n",
    "        batch = batch.cuda()\n",
    "        features = pool5_extractor(batch)\n",
    "        features = features.detach().cpu()\n",
    "        features_array.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4853, 1024, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4853, 1024, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of features to a NumPy array\n",
    "features_array = torch.cat(features_array).numpy()\n",
    "print(features_array.shape)\n",
    "\n",
    "# Save the features to a file (you can use any preferred method, e.g., np.save)\n",
    "np.save('features.npy', features_array)\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4853, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_array = np.load('features.npy')\n",
    "\n",
    "# remove the third and fourth dimensions\n",
    "features_array = np.squeeze(features_array, axis=2)\n",
    "features_array = np.squeeze(features_array, axis=2)\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_compress.cpu().eval();\n",
    "summarizer.cpu().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "with torch.no_grad():\n",
    "    features_array = torch.from_numpy(features_array).float().cpu()\n",
    "\n",
    "    video_tensor = features_array.view(-1, 1, input_size)\n",
    "    video_feature = Variable(video_tensor).cpu()\n",
    "\n",
    "    # [seq_len, 1, hidden_size]\n",
    "    video_feature = linear_compress(video_feature.detach()).unsqueeze(1)\n",
    "    \n",
    "    scores = summarizer.attentive_selector(video_feature).squeeze(1)\n",
    "    scores = scores.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video_1',\n",
       " 'video_11',\n",
       " 'video_12',\n",
       " 'video_15',\n",
       " 'video_16',\n",
       " 'video_36',\n",
       " 'video_42',\n",
       " 'video_5',\n",
       " 'video_50',\n",
       " 'video_6']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from evaluation.generate_summary import generate_summary\n",
    "from evaluation.evaluation_metrics import evaluate_summary\n",
    "import cv2\n",
    "\n",
    "\n",
    "PATH_TVSum = \"data/TVSum/eccv16_dataset_tvsum_google_pool5.h5\"\n",
    "model_predictions = \"model/output/attentive/base/tvsum/results/split4/tvsum_40.json\"\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "data = json.loads(open(model_predictions).read())\n",
    "keys = list(data.keys())\n",
    "\n",
    "for video_name in keys:\n",
    "    scores = np.asarray(data[video_name])\n",
    "    all_scores.append(scores)\n",
    "\n",
    "all_user_summary, all_shot_bound, all_nframes, all_positions = [], [], [], []\n",
    "with h5py.File(PATH_TVSum, \"r\") as hdf:\n",
    "    for video_name in keys:\n",
    "        video_index = video_name[6:]\n",
    "\n",
    "        user_summary = np.array(\n",
    "            hdf.get(\"video_\" + video_index + \"/user_summary\")\n",
    "        )\n",
    "        sb = np.array(hdf.get(\"video_\" + video_index + \"/change_points\"))\n",
    "        n_frames = np.array(hdf.get(\"video_\" + video_index + \"/n_frames\"))\n",
    "        positions = np.array(hdf.get(\"video_\" + video_index + \"/picks\"))\n",
    "\n",
    "        all_user_summary.append(user_summary)\n",
    "        all_shot_bound.append(sb)\n",
    "        all_nframes.append(n_frames)\n",
    "        all_positions.append(positions)\n",
    "\n",
    "all_summaries = generate_summary(\n",
    "    all_shot_bound, all_scores, all_nframes, all_positions\n",
    ")\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'video_1'\n",
    "video_idx = int(video_name[6:]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AwmHb44_ouw\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tvsum_video_dir = \"tvsum50_ver_1_1/ydata-tvsum50-v1_1/ydata-tvsum50-video/video/\"\n",
    "tvsum_info_file = \"tvsum50_ver_1_1/ydata-tvsum50-v1_1/ydata-tvsum50-data/data/ydata-tvsum50-info.tsv\"\n",
    "tvsum_info = pd.read_csv(tvsum_info_file, sep=\"\\t\")\n",
    "video_id = tvsum_info[\"video_id\"][video_idx]\n",
    "print(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_summary = all_summaries[keys.index(video_name)]\n",
    "video_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = os.path.join(tvsum_video_dir, video_id + \".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "original_frames = []\n",
    "\n",
    "\n",
    "# Process each frame in the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break  # Break the loop if no more frames\n",
    "    \n",
    "    original_frames.append(frame)\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seleted_frames = []\n",
    "for i, frame in enumerate(original_frames):\n",
    "    if video_summary[i] == 1:\n",
    "        seleted_frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video length: 10,597\n",
      "Selected frames length: 1,587\n",
      "len(seleted_frames) / len(original_frames): 0.14975936585826177\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original video length: {len(original_frames):,}\")\n",
    "print(f\"Selected frames length: {len(seleted_frames):,}\")\n",
    "print(f'len(seleted_frames) / len(original_frames): {len(seleted_frames) / len(original_frames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_writer = cv2.VideoWriter(\n",
    "    f\"summary_{video_id}.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    60,\n",
    "    (original_width, original_height),\n",
    ")\n",
    "\n",
    "for frame in seleted_frames:\n",
    "    video_writer.write(frame)\n",
    "    \n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
